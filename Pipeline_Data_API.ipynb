{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5979f340",
   "metadata": {},
   "source": [
    "The following libraries are loaded to support API communication, weather data retrieval, GRIB file processing, and general data manipulation for the ERA5, Fingrid, Esett and EnergiDataService workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ed1998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import cdsapi\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78029c6",
   "metadata": {},
   "source": [
    "## Fingrid â€“ API Data Retrieval\n",
    "\n",
    "The code below downloads Finnish electricity system data from the **Fingrid Open Data API**.  \n",
    "Authentication is required, so the script loads an API key from the environment and includes it in every request via the `x-api-key` header.\n",
    "\n",
    "Because the Fingrid API has request-size limitations, the script retrieves data **month by month**. For each year (2023 and 2024), monthly time windows are generated, and the API is queried repeatedly until all pages of data for that month are fetched. The script automatically handles pagination using the metadata returned by Fingrid.\n",
    "\n",
    "The following Fingrid datasets are downloaded:\n",
    "\n",
    "- **Consumption** (dataset 192)  \n",
    "- **Wind generation** (dataset 181)  \n",
    "- **Nuclear generation** (dataset 188)  \n",
    "- **Hydro generation** (dataset 191)\n",
    "\n",
    "For each dataset and each year, all monthly records are combined, converted to a unified timestamp (`datetime_utc`), cleaned, deduplicated, and sorted before being saved as a yearly CSV file, such as:\n",
    "\n",
    "- `fi_consumption_2023.csv`  \n",
    "- `fi_consumption_2024.csv`\n",
    "- `etc.`\n",
    "\n",
    "This ensures a complete and continuous time series for both 2023 and 2024, despite the Fingrid API returning data in paginated batches and limiting the size of individual requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fa857ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading consumption for year 2023\n",
      "  â†’ consumption | 2023-01-01T00:00:00Z â†’ 2023-02-01T00:00:00Z\n",
      "  â†’ consumption | 2023-02-01T00:00:00Z â†’ 2023-03-01T00:00:00Z\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 97\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, dataset_id \u001b[38;5;129;01min\u001b[39;00m FINGRID_DATASETS\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 97\u001b[0m         \u001b[43mdownload_dataset_year\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2023\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m         download_dataset_year(dataset_id, name, \u001b[38;5;241m2024\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 46\u001b[0m, in \u001b[0;36mdownload_dataset_year\u001b[0;34m(dataset_id, name, year)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m         data_json \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_fingrid_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m         data \u001b[38;5;241m=\u001b[39m data_json\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n",
      "Cell \u001b[0;32mIn[11], line 30\u001b[0m, in \u001b[0;36mfetch_fingrid_dataset\u001b[0;34m(dataset_id, start, end, page, page_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx-api-key\u001b[39m\u001b[38;5;124m\"\u001b[39m: FINGRID_API_KEY}\n\u001b[1;32m     23\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartTime\u001b[39m\u001b[38;5;124m\"\u001b[39m: start,\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendTime\u001b[39m\u001b[38;5;124m\"\u001b[39m: end,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m }\n\u001b[0;32m---> 30\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/anaconda3/envs/nordic-energy/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nordic-energy/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nordic-energy/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/nordic-energy/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/nordic-energy/lib/python3.10/site-packages/requests/adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/nordic-energy/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nordic-energy/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/nordic-energy/lib/python3.10/site-packages/urllib3/connection.py:565\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    562\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/anaconda3/envs/nordic-energy/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/nordic-energy/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nordic-energy/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nordic-energy/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nordic-energy/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/nordic-energy/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "FINGRID_API_KEY = os.getenv(\"FINGRID_API_KEY\")\n",
    "\n",
    "BASE_URL = \"https://data.fingrid.fi/api/datasets\"\n",
    "\n",
    "def generate_monthly_ranges_for_year(year):\n",
    "    \"\"\"Generate month-to-month intervals for a specific year.\"\"\"\n",
    "    start = f\"{year}-01-01\"\n",
    "    end = f\"{year+1}-01-01\"\n",
    "    dates = pd.date_range(start=start, end=end, freq=\"MS\")\n",
    "\n",
    "    return [\n",
    "        (\n",
    "            dates[i].strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "            dates[i + 1].strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        )\n",
    "        for i in range(len(dates) - 1)\n",
    "    ]\n",
    "def fetch_fingrid_dataset(dataset_id, start, end, page=1, page_size=20000):\n",
    "    \"\"\"Fetch a single page of a Fingrid dataset.\"\"\"\n",
    "    url = f\"{BASE_URL}/{dataset_id}/data\"\n",
    "    headers = {\"x-api-key\": FINGRID_API_KEY}\n",
    "    params = {\n",
    "        \"startTime\": start,\n",
    "        \"endTime\": end,\n",
    "        \"page\": page,\n",
    "        \"pageSize\": page_size,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def download_dataset_year(dataset_id, name, year):\n",
    "    \"\"\"Download a full Fingrid dataset for one year.\"\"\"\n",
    "    print(f\"\\nDownloading {name} for year {year}\")\n",
    "    all_data = []\n",
    "\n",
    "    for start, end in generate_monthly_ranges_for_year(year):\n",
    "        print(f\"  â†’ {name} | {start} â†’ {end}\")\n",
    "        page = 1\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                data_json = fetch_fingrid_dataset(dataset_id, start, end, page)\n",
    "                data = data_json.get(\"data\", [])\n",
    "\n",
    "                if not data:\n",
    "                    break\n",
    "\n",
    "                df = pd.DataFrame(data)\n",
    "                df[\"datetime_utc\"] = (\n",
    "                    pd.to_datetime(df[\"startTime\"], utc=True)\n",
    "                      .dt.tz_localize(None)\n",
    "                )\n",
    "                df = df[[\"datetime_utc\", \"value\"]]\n",
    "\n",
    "                all_data.append(df)\n",
    "\n",
    "                pagination = data_json.get(\"pagination\", {})\n",
    "                if page >= pagination.get(\"lastPage\", 1):\n",
    "                    break\n",
    "\n",
    "                page += 1\n",
    "                time.sleep(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during {start}, page {page}: {e}\")\n",
    "                break\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "    if all_data:\n",
    "        final_df = (\n",
    "            pd.concat(all_data)\n",
    "              .drop_duplicates()\n",
    "              .sort_values(\"datetime_utc\")\n",
    "        )\n",
    "\n",
    "        os.makedirs(\"../data\", exist_ok=True)\n",
    "        filename = f\"../data/fi_{name}_{year}.csv\"\n",
    "        final_df.to_csv(filename, index=False)\n",
    "\n",
    "        print(f\"Saved {filename} with {len(final_df)} rows\")\n",
    "\n",
    "FINGRID_DATASETS = {\n",
    "    \"consumption\": 192,\n",
    "    \"wind\": 181,\n",
    "    \"nuclear\": 188,\n",
    "    \"hydro\": 191,\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for name, dataset_id in FINGRID_DATASETS.items():\n",
    "        download_dataset_year(dataset_id, name, 2023)\n",
    "        download_dataset_year(dataset_id, name, 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec87af",
   "metadata": {},
   "source": [
    "Esett API â€“ Download Electricity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83633539",
   "metadata": {},
   "outputs": [],
   "source": [
    "MBA_CODES = {\n",
    "    \"SE1\": \"10Y1001A1001A44P\",\n",
    "    \"SE2\": \"10Y1001A1001A45N\",\n",
    "    \"SE3\": \"10Y1001A1001A46L\",\n",
    "    \"SE4\": \"10Y1001A1001A47J\",\n",
    "    \"FI\":  \"10YFI_1________U\",\n",
    "    \"DK1\": \"10YDK-1--------W\",\n",
    "    \"DK2\": \"10YDK-2--------M\",\n",
    "    \"NO1\": \"10YNO_1________2\",\n",
    "    \"NO2\": \"10YNO_2________T\",\n",
    "    \"NO3\": \"10YNO_3________J\",\n",
    "    \"NO4\": \"10YNO_4________9\",\n",
    "    \"NO5\": \"10Y1001A1001A48H\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "BASE_URL_ESETT = \"https://api.opendata.esett.com\"\n",
    "SAVE_PATH = \"../data/esett\"\n",
    "\n",
    "def generate_months(start=\"2024-01-01\", end=\"2025-01-01\"):\n",
    "    dates = pd.date_range(start=start, end=end, freq=\"MS\")\n",
    "    return [(dates[i], dates[i + 1]) for i in range(len(dates) - 1)]\n",
    "\n",
    "def fetch_esett(endpoint, mba, start, end):\n",
    "    params = {\n",
    "        \"start\": start.strftime(\"%Y-%m-%dT00:00:00.000Z\"),\n",
    "        \"end\":   end.strftime(\"%Y-%m-%dT00:00:00.000Z\"),\n",
    "        \"mba\":   mba\n",
    "    }\n",
    "    url = BASE_URL_ESETT + endpoint\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    if r.status_code == 204:\n",
    "        return None\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def normalize_df(df, zone, value_col=\"total\"):\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    if \"timestampUTC\" in df:\n",
    "        df[\"datetime_utc\"] = pd.to_datetime(df[\"timestampUTC\"], utc=True)\n",
    "    elif \"timestamp\" in df:\n",
    "        df[\"datetime_utc\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
    "    else:\n",
    "        return None\n",
    "    if value_col not in df.columns:\n",
    "        value_col = \"value\"\n",
    "    df = df[[\"datetime_utc\", value_col]].rename(columns={value_col: zone})\n",
    "    df[\"datetime_utc\"] = df[\"datetime_utc\"].dt.tz_localize(None)\n",
    "    return df\n",
    "\n",
    "def download_dataset_all_zones(exp_endpoint, output_name, value_field):\n",
    "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "    full_df = None\n",
    "\n",
    "    for zone, mba in MBA_CODES.items():\n",
    "        zone_dfs = []\n",
    "        print(f\"ðŸ”µ {output_name} | {zone}\")\n",
    "        for (start, end) in generate_months():\n",
    "            try:\n",
    "                df = fetch_esett(exp_endpoint, mba, start, end)\n",
    "                df = normalize_df(df, zone, value_field)\n",
    "                if df is not None:\n",
    "                    zone_dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"   âš  Error {zone} {start}: {e}\")\n",
    "            time.sleep(1.5)\n",
    "\n",
    "        if zone_dfs:\n",
    "            zone_data = pd.concat(zone_dfs).drop_duplicates().sort_values(\"datetime_utc\")\n",
    "            if full_df is None:\n",
    "                full_df = zone_data\n",
    "            else:\n",
    "                full_df = full_df.merge(zone_data, on=\"datetime_utc\", how=\"outer\")\n",
    "\n",
    "    if full_df is not None:\n",
    "        full_df.to_csv(f\"{SAVE_PATH}/{output_name}_2024.csv\", index=False)\n",
    "        print(f\"âœ… Saved {output_name}_2024.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸš€ Downloading Production (EXP16)\")\n",
    "    download_dataset_all_zones(\"/EXP16/Aggregate\", \"production\", \"total\")\n",
    "\n",
    "    print(\"ðŸš€ Downloading Consumption (EXP15)\")\n",
    "    download_dataset_all_zones(\"/EXP15/Aggregate\", \"consumption\", \"total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL_ESETT= \"https://api.opendata.esett.com/EXP13/Aggregate\"\n",
    "SAVE_PATH = \"../data/esett\"\n",
    "\n",
    "\n",
    "\n",
    "def generate_months(start=\"2024-01-01\", end=\"2025-01-01\"):\n",
    "    dates = pd.date_range(start=start, end=end, freq=\"MS\")\n",
    "    return [(dates[i], dates[i + 1]) for i in range(len(dates) - 1)]\n",
    "\n",
    "def fetch_imbalance(mba_code, start, end):\n",
    "    params = {\n",
    "        \"start\": start.strftime(\"%Y-%m-%dT00:00:00.000Z\"),\n",
    "        \"end\":   end.strftime(\"%Y-%m-%dT00:00:00.000Z\"),\n",
    "        \"mba\":   mba_code\n",
    "    }\n",
    "    r = requests.get(BASE_URL_ESETT, params=params, timeout=30)\n",
    "    if r.status_code == 204:\n",
    "        return None\n",
    "    r.raise_for_status()\n",
    "    return pd.DataFrame(r.json())\n",
    "\n",
    "def normalize(df, zone):\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    if \"timestampUTC\" in df.columns:\n",
    "        df[\"datetime_utc\"] = pd.to_datetime(df[\"timestampUTC\"], utc=True).dt.tz_localize(None)\n",
    "    elif \"timestamp\" in df.columns:\n",
    "        df[\"datetime_utc\"] = pd.to_datetime(df[\"timestamp\"], utc=True).dt.tz_localize(None)\n",
    "    else:\n",
    "        return None\n",
    "    if \"imbalance\" not in df.columns:\n",
    "        return None\n",
    "    return df[[\"datetime_utc\", \"imbalance\"]].rename(columns={\"imbalance\": zone})\n",
    "\n",
    "def download_all_imbalance():\n",
    "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "    full_df = None\n",
    "\n",
    "    for zone, mba in MBA_CODES.items():\n",
    "        print(f\"ðŸ”µ Fetching Imbalance for {zone}\")\n",
    "        zone_monthly = []\n",
    "\n",
    "        for start, end in generate_months():\n",
    "            try:\n",
    "                df = fetch_imbalance(mba, start, end)\n",
    "                df = normalize(df, zone)\n",
    "                if df is not None:\n",
    "                    zone_monthly.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"âš  Error for {zone} at {start}: {e}\")\n",
    "            time.sleep(1.2)\n",
    "\n",
    "        if zone_monthly:\n",
    "            zone_df = pd.concat(zone_monthly).drop_duplicates().sort_values(\"datetime_utc\")\n",
    "            if full_df is None:\n",
    "                full_df = zone_df\n",
    "            else:\n",
    "                full_df = full_df.merge(zone_df, on=\"datetime_utc\", how=\"outer\")\n",
    "\n",
    "    if full_df is not None:\n",
    "        file_path = f\"{SAVE_PATH}/imbalance_2024.csv\"\n",
    "        full_df.to_csv(file_path, index=False)\n",
    "        print(f\"Saved {file_path} with shape: {full_df.shape}\")\n",
    "    else:\n",
    "        print(\"No data saved!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_all_imbalance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27370637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eSett API endpoint for single balance prices\n",
    "BASE_URL_ESETT = \"https://api.opendata.esett.com/EXP14/Aggregate\"\n",
    "SAVE_PATH = \"../data/esett\"\n",
    "\n",
    "def generate_months(start=\"2024-01-01\", end=\"2025-01-01\"):\n",
    "    dates = pd.date_range(start=start, end=end, freq=\"MS\")\n",
    "    return [(dates[i], dates[i + 1]) for i in range(len(dates)-1)]\n",
    "\n",
    "def fetch_prices(mba_code, start, end):\n",
    "    params = {\n",
    "        \"start\": start.strftime(\"%Y-%m-%dT00:00:00.000Z\"),\n",
    "        \"end\": end.strftime(\"%Y-%m-%dT00:00:00.000Z\"),\n",
    "        \"mba\": mba_code,\n",
    "        \"resolution\": \"hour\"\n",
    "    }\n",
    "    resp = requests.get(BASE_URL_ESETT, params=params)\n",
    "    if resp.status_code == 204:\n",
    "        return None\n",
    "    resp.raise_for_status()\n",
    "    return pd.DataFrame(resp.json())\n",
    "\n",
    "def normalize_price_df(df, zone):\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "\n",
    "    if \"timestampUTC\" in df:\n",
    "        df[\"datetime_utc\"] = pd.to_datetime(df[\"timestampUTC\"], utc=True).dt.tz_localize(None)\n",
    "    else:\n",
    "        df[\"datetime_utc\"] = pd.to_datetime(df[\"timestamp\"], utc=True).dt.tz_localize(None)\n",
    "\n",
    "    return df[[\"datetime_utc\", \"upRegPrice\", \"downRegPrice\"]].rename(\n",
    "        columns={\n",
    "            \"upRegPrice\": f\"{zone}_up\",\n",
    "            \"downRegPrice\": f\"{zone}_down\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "def download_all_prices():\n",
    "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "    full_df = None\n",
    "\n",
    "    for zone, mba in MBA_CODES.items():\n",
    "        zone_months = []\n",
    "        print(f\"Fetching EXP14 Prices for {zone}\")\n",
    "\n",
    "        for start, end in generate_months():\n",
    "            try:\n",
    "                df = fetch_prices(mba, start, end)\n",
    "                df = normalize_price_df(df, zone)\n",
    "                if df is not None:\n",
    "                    zone_months.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error at {start} for {zone}: {e}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        if zone_months:\n",
    "            zone_data = pd.concat(zone_months).drop_duplicates().sort_values(\"datetime_utc\")\n",
    "            if full_df is None:\n",
    "                full_df = zone_data\n",
    "            else:\n",
    "                full_df = full_df.merge(zone_data, on=\"datetime_utc\", how=\"outer\")\n",
    "\n",
    "    if full_df is not None:\n",
    "        file_path = f\"{SAVE_PATH}/balancing_price_2024.csv\"\n",
    "        full_df.to_csv(file_path, index=False)\n",
    "        print(f\"Saved balancing_price_2024.csv with shape {full_df.shape}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_all_prices()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a2c09",
   "metadata": {},
   "source": [
    "## EnergiDataService â€“ Elspot Price Retrieval\n",
    "\n",
    "The script below downloads hourly Elspot electricity prices from the **EnergiDataService API** (`api.energidataservice.dk`). A list of Nordic bidding zones is defined, and the function `get_elspot_prices()` sends an API request for each zone using the specified date range. No API key is required for this dataset, so all requests are made anonymously.\n",
    "\n",
    "For each bidding zone, the API response is converted into a pandas DataFrame containing the timestamp (`HourUTC`), bidding zone (`PriceArea`), and hourly spot price in euros (`SpotPriceEUR`). The function `download_all_nordic_prices()` loops through all Nordic zones and saves each result as a separate CSV file.\n",
    "\n",
    "Although the script queries **all Nordic bidding zones** (DK1, DK2, NO1â€“NO5, SE1â€“SE4, FI), the EnergiDataService Elspot dataset only contains price data for a limited set of areas. As a result, **the output files are only generated for**:\n",
    "\n",
    "- **DK1**  \n",
    "- **DK2**  \n",
    "- **NO2**  \n",
    "- **SE3**  \n",
    "- **SE4**\n",
    "\n",
    "For all other bidding zones, the API returns an empty dataset, and no CSV file is saved. This behavior reflects the coverage of the EnergiDataService platform, which does not provide Elspot price data for all Nordic areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738866e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data folder if it doesn't exist\n",
    "os.makedirs(\"../data/energi\", exist_ok=True)\n",
    "\n",
    "# List of Nordic bidding zones\n",
    "NORDIC_ZONES = [\n",
    "    \"DK1\", \"DK2\",\n",
    "    \"NO1\", \"NO2\", \"NO3\", \"NO4\", \"NO5\",\n",
    "    \"SE1\", \"SE2\", \"SE3\", \"SE4\",\n",
    "    \"FI\"\n",
    "]\n",
    "\n",
    "def get_elspot_prices(start=\"2024-01-01\", end=\"2025-01-01\", area=\"DK1\"):\n",
    "    \"\"\"Fetch Elspot prices from Energi Data Service for a given price area.\"\"\"\n",
    "    url = \"https://api.energidataservice.dk/dataset/Elspotprices\"\n",
    "    params = {\n",
    "        \"start\": start,\n",
    "        \"end\": end,\n",
    "        \"filter\": f'{{\"PriceArea\": [\"{area}\"]}}'\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to pull data for {area}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "    data = response.json().get(\"records\", [])\n",
    "    if not data:\n",
    "        print(f\"No data returned for {area}\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"HourUTC\"] = pd.to_datetime(df[\"HourUTC\"])\n",
    "    df = df[[\"HourUTC\", \"PriceArea\", \"SpotPriceEUR\"]]\n",
    "    return df\n",
    "\n",
    "def download_all_nordic_prices(start=\"2024-01-01\", end=\"2025-01-01\"):\n",
    "    for zone in NORDIC_ZONES:\n",
    "        print(f\"â¬‡ Downloading price data for {zone} ...\")\n",
    "        df = get_elspot_prices(start, end, zone)\n",
    "\n",
    "        if df is not None and not df.empty:\n",
    "            filepath = f\"/data/{zone.lower()}_prices.csv\"\n",
    "            df.to_csv(filepath, index=False)\n",
    "            print(f\"Saved: {filepath}\")\n",
    "        else:\n",
    "            print(f\"No data saved for {zone}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_all_nordic_prices(\"2024-01-01\", \"2025-01-01\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c47d9b",
   "metadata": {},
   "source": [
    "## Weather â€“ CDS API\n",
    "\n",
    "The code below sends a request to the **CDS API** to download ERA5 reanalysis data for a selected month and year. The request specifies the required meteorological variables (temperature, wind components at 10 m and 100 m, mean sea-level pressure, precipitation, and surface solar radiation) as well as all days and hours in the chosen period, together with a bounding box covering the entire Nordic region.\n",
    "\n",
    "Because the CDS API has **request size limitations**, the fields **`year`** and **`month`** in the request must be **manually adjusted** each time to retrieve the next month of data. The output filename (e.g., `era5_weather_2023_01.grib`) must also be updated manually to match the selected period. After these adjustments, running the cell will download and save the corresponding GRIB file for that month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf67f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cdsapi.Client()\n",
    "\n",
    "dataset = \"reanalysis-era5-single-levels\"\n",
    "\n",
    "request = {\n",
    "    \"product_type\": \"reanalysis\",\n",
    "    \"variable\": [\n",
    "        \"2m_temperature\",\n",
    "        \"10m_u_component_of_wind\",\n",
    "        \"10m_v_component_of_wind\",\n",
    "        \"mean_sea_level_pressure\",\n",
    "        \"total_precipitation\",\n",
    "        \"100m_u_component_of_wind\",\n",
    "        \"100m_v_component_of_wind\",\n",
    "        \"surface_solar_radiation_downwards\"\n",
    "    ],\n",
    "\n",
    "    \"year\": [\"2023\"],\n",
    "    \"month\": [\"01\"],\n",
    "    \"day\": [f\"{d:02d}\" for d in range(1, 32)],\n",
    "    \"time\": [f\"{h:02d}:00\" for h in range(24)],\n",
    "\n",
    "    # Entire Nordic region bounding box (North, West, South, East)\n",
    "    \"area\": [72, 5, 54, 32],\n",
    "\n",
    "    \"format\": \"grib\"\n",
    "}\n",
    "\n",
    "client.retrieve(dataset, request).download(\"era5_weather_2023_01.grib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d3947",
   "metadata": {},
   "source": [
    "# Weather â€“ Bidding Zone Extraction\n",
    "\n",
    "After downloading the monthly ERA5 GRIB files, the script below processes each file and extracts weather values for all Nordic bidding zones. Each GRIB file is opened using `xarray` with the `cfgrib` engine, and the weather variables are sampled at a representative latitudeâ€“longitude coordinate for each bidding zone (e.g., DK1, DK2, NO1â€“NO5, SE1â€“SE4, FI). The script selects the **nearest ERA5 grid point** to each zoneâ€™s coordinate and converts the resulting dataset into a pandas DataFrame.\n",
    "\n",
    "Only the relevant weather variables (temperature, wind components, pressure, precipitation, and solar radiation) are kept. The temperature variable is also converted from Kelvin to Celsius. Finally, the processed data for all zones is saved as a monthly CSV file named `bidding_zone_weather_YYYY_MM.csv`.\n",
    "\n",
    "This allows each month of ERA5 climate data to be transformed into a clean, zone-level dataset aligned with the rest of the Nordic electricity and balancing datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254ca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2023/era5_weather_2023_01.grib\n",
      "[2023-02] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2023/era5_weather_2023_02.grib\n",
      "[2023-03] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2023/era5_weather_2023_03.grib\n",
      "[2023-04] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2023/era5_weather_2023_04.grib\n",
      "[2023-05] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2023/era5_weather_2023_05.grib\n",
      "[2023-06] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2023/era5_weather_2023_06.grib\n",
      "[2023-07] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2023/era5_weather_2023_07.grib\n",
      "[2023-08] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2023/era5_weather_2023_08.grib\n",
      "[2023-09] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2023/era5_weather_2023_09.grib\n",
      "[2023-10] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2023/era5_weather_2023_10.grib\n",
      "[2023-11] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2023/era5_weather_2023_11.grib\n",
      "[2023-12] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2023/era5_weather_2023_12.grib\n",
      "[2024-01] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2024/era5_weather_2024_01.grib\n",
      "[2024-02] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2024/era5_weather_2024_02.grib\n",
      "[2024-03] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2024/era5_weather_2024_03.grib\n",
      "[2024-04] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2024/era5_weather_2024_04.grib\n",
      "[2024-05] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2024/era5_weather_2024_05.grib\n",
      "[2024-06] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2024/era5_weather_2024_06.grib\n",
      "[2024-07] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2024/era5_weather_2024_07.grib\n",
      "[2024-08] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2024/era5_weather_2024_08.grib\n",
      "[2024-09] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2024/era5_weather_2024_09.grib\n",
      "[2024-10] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2024/era5_weather_2024_10.grib\n",
      "[2024-11] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2024/era5_weather_2024_11.grib\n",
      "[2024-12] Ð¤Ð°Ð¹Ð» Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: Weather data/2024/era5_weather_2024_12.grib\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path(\"Weather data\") \n",
    "years = [2023, 2024]\n",
    "months = range(1, 13)\n",
    "\n",
    "bidding_zone_points = {\n",
    "    \"DK1\": (55.6, 9.2),\n",
    "    \"DK2\": (55.7, 12.5),\n",
    "    \"NO1\": (60.0, 10.0),\n",
    "    \"NO2\": (59.0, 6.5),\n",
    "    \"NO3\": (64.0, 11.0),\n",
    "    \"NO4\": (69.0, 19.0),\n",
    "    \"NO5\": (62.0, 5.5),\n",
    "    \"SE1\": (66.0, 20.0),\n",
    "    \"SE2\": (63.0, 17.0),\n",
    "    \"SE3\": (59.5, 16.0),\n",
    "    \"SE4\": (57.0, 15.0),\n",
    "    \"FI\":  (61.5, 25.0),\n",
    "}\n",
    "\n",
    "for year in years:\n",
    "    year_dir = base_dir / str(year)\n",
    "\n",
    "    for month in months:\n",
    "        grib_name = f\"era5_weather_{year}_{month:02d}.grib\"\n",
    "        grib_path = year_dir / grib_name\n",
    "\n",
    "        if not grib_path.exists():\n",
    "            print(f\"[{year}-{month:02d}] File not found: {grib_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing {grib_path}\")\n",
    "\n",
    "        ds = xr.open_dataset(\n",
    "            grib_path,\n",
    "            engine=\"cfgrib\",\n",
    "            backend_kwargs={\"indexpath\": \"\"}\n",
    "        )\n",
    "        print(\"Dataset loaded\")\n",
    "\n",
    "        zone_dataframes = []\n",
    "\n",
    "        for zone, (lat, lon) in bidding_zone_points.items():\n",
    "            print(f\" -> {zone}: lat={lat}, lon={lon}\")\n",
    "            point = ds.sel(latitude=lat, longitude=lon, method=\"nearest\")\n",
    "            df_zone = point.to_dataframe().reset_index()\n",
    "            df_zone[\"zone\"] = zone\n",
    "            zone_dataframes.append(df_zone)\n",
    "\n",
    "        df_all = pd.concat(zone_dataframes, ignore_index=True)\n",
    "\n",
    "        cols_to_keep = [\"valid_time\", \"zone\"]\n",
    "        for col in [\"t2m\", \"u10\", \"v10\", \"msl\", \"tp\", \"ssrd\", \"u100\", \"v100\"]:\n",
    "            if col in df_all.columns:\n",
    "                cols_to_keep.append(col)\n",
    "\n",
    "        df_all = df_all[cols_to_keep]\n",
    "\n",
    "        if \"t2m\" in df_all.columns:\n",
    "            df_all[\"t2m\"] = df_all[\"t2m\"] - 273.15\n",
    "\n",
    "        csv_name = f\"bidding_zone_weather_{year}_{month:02d}.csv\"\n",
    "        csv_path = year_dir / csv_name\n",
    "\n",
    "        df_all.to_csv(csv_path, index=False)\n",
    "        print(f\"Saved: {csv_path}\")\n",
    "        print(df_all.head())\n",
    "        print(df_all.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nordic-energy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
