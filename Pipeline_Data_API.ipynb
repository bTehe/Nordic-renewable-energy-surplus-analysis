{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e0a25f",
   "metadata": {},
   "source": [
    "# Data pipelines (API downloads)\n",
    "\n",
    "This notebook fetches raw data from external APIs and writes them into the repo's folder layout:\n",
    "- Fingrid: `Dataset/fin/<year>/fi_<dataset>_<year>.csv`\n",
    "- eSett open data: `Dataset/esett/<year>/*.csv`\n",
    "- Energi Data Service day-ahead prices:  `Dataset/energi/<year>/<zone>_prices.csv`\n",
    "- ERA5 weather (GRIB): `Raw data/<year>/era5_weather_<year>_<month>.grib`, extracted to `Weather data/<year>/bidding_zone_weather_<year>_<month>.csv`\n",
    "\n",
    "Environment: ensure `.env` exists with `FINGRID_API_KEY=<your_key>` (get one from Fingrid Open Data portal).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd30ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import xarray as xr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    import cdsapi\n",
    "except ImportError:\n",
    "    cdsapi = None\n",
    "\n",
    "load_dotenv(Path('.env') / '.env')\n",
    "FINGRID_API_KEY = os.getenv('FINGRID_API_KEY')\n",
    "\n",
    "DATASET_DIR = Path('Dataset')\n",
    "RAW_WEATHER_DIR = Path('Raw data')\n",
    "WEATHER_OUT_DIR = Path('Weather data')\n",
    "\n",
    "NORDIC_ZONES = ['DK1','DK2','NO1','NO2','NO3','NO4','NO5','SE1','SE2','SE3','SE4','FI']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b136b",
   "metadata": {},
   "source": [
    "## Fingrid (FI generation/consumption)\n",
    "- Auth via `FINGRID_API_KEY` in `.env`\n",
    "- Fetches monthly pages for 2023/2024\n",
    "- Saves to `Dataset/fin/<year>/fi_<dataset>_<year>.csv`\n",
    "Datasets: consumption (192), wind (181), nuclear (188), hydro (191)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59252f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINGRID_DATASETS = {\n",
    "    'consumption': 192,\n",
    "    'wind': 181,\n",
    "    'nuclear': 188,\n",
    "    'hydro': 191,\n",
    "}\n",
    "\n",
    "BASE_URL_FINGRID = \"https://data.fingrid.fi/api/datasets\"\n",
    "\n",
    "\n",
    "def monthly_ranges(year: int) -> List[Tuple[str, str]]:\n",
    "    start = f\"{year}-01-01\"\n",
    "    end = f\"{year+1}-01-01\"\n",
    "    dates = pd.date_range(start=start, end=end, freq='MS')\n",
    "    return [(\n",
    "        dates[i].strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "        dates[i+1].strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    ) for i in range(len(dates)-1)]\n",
    "\n",
    "\n",
    "def fetch_fingrid(dataset_id: int, start: str, end: str, page: int = 1, page_size: int = 20000) -> dict:\n",
    "    headers = {'x-api-key': FINGRID_API_KEY}\n",
    "    params = {\n",
    "        'startTime': start,\n",
    "        'endTime': end,\n",
    "        'page': page,\n",
    "        'pageSize': page_size,\n",
    "        'format': 'json',\n",
    "    }\n",
    "    r = requests.get(f\"{BASE_URL_FINGRID}/{dataset_id}/data\", headers=headers, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "def download_fingrid_year(dataset_id: int, name: str, year: int) -> None:\n",
    "    out_dir = DATASET_DIR / 'fin' / str(year)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    parts = []\n",
    "    for start, end in monthly_ranges(year):\n",
    "        page = 1\n",
    "        while True:\n",
    "            data_json = fetch_fingrid(dataset_id, start, end, page)\n",
    "            data = data_json.get('data', [])\n",
    "            if not data:\n",
    "                break\n",
    "            df = pd.DataFrame(data)\n",
    "            df['datetime_utc'] = pd.to_datetime(df['startTime'], utc=True).dt.tz_localize(None)\n",
    "            df = df[['datetime_utc','value']]\n",
    "            parts.append(df)\n",
    "            last_page = data_json.get('pagination', {}).get('lastPage', page)\n",
    "            if page >= last_page:\n",
    "                break\n",
    "            page += 1\n",
    "            time.sleep(0.2)\n",
    "    if parts:\n",
    "        final_df = pd.concat(parts).drop_duplicates().sort_values('datetime_utc')\n",
    "        dest = out_dir / f\"fi_{name}_{year}.csv\"\n",
    "        final_df.to_csv(dest, index=False)\n",
    "        print(f\"Saved {dest} ({len(final_df)} rows)\")\n",
    "    else:\n",
    "        print(f\"No data for {name} {year}\")\n",
    "\n",
    "RUN_FINGRID = False\n",
    "if RUN_FINGRID:\n",
    "    for name, dsid in FINGRID_DATASETS.items():\n",
    "        for yr in [2023, 2024]:\n",
    "            download_fingrid_year(dsid, name, yr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e0d28",
   "metadata": {},
   "source": [
    "## eSett open data (production, consumption, imbalance, balancing prices)\n",
    "- Endpoints EXP16 (production), EXP15 (consumption), EXP13 (imbalance), EXP14 (balancing prices)\n",
    "- Saved to `Dataset/esett/<year>/...`\n",
    "- Loops zones via MBA codes and months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccbdf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "MBA_CODES = {\n",
    "    'SE1': '10Y1001A1001A44P',\n",
    "    'SE2': '10Y1001A1001A45N',\n",
    "    'SE3': '10Y1001A1001A46L',\n",
    "    'SE4': '10Y1001A1001A47J',\n",
    "    'FI':  '10YFI_1________U',\n",
    "    'DK1': '10YDK-1--------W',\n",
    "    'DK2': '10YDK-2--------M',\n",
    "    'NO1': '10YNO_1________2',\n",
    "    'NO2': '10YNO_2________T',\n",
    "    'NO3': '10YNO_3________J',\n",
    "    'NO4': '10YNO_4________9',\n",
    "    'NO5': '10Y1001A1001A48H',\n",
    "}\n",
    "\n",
    "ESETT_BASE = \"https://api.opendata.esett.com\"\n",
    "\n",
    "def month_edges(year: int):\n",
    "    dates = pd.date_range(start=f\"{year}-01-01\", end=f\"{year+1}-01-01\", freq='MS')\n",
    "    for i in range(len(dates)-1):\n",
    "        yield dates[i], dates[i+1]\n",
    "\n",
    "\n",
    "def fetch_esett(endpoint: str, mba: str, start, end) -> pd.DataFrame | None:\n",
    "    params = {\n",
    "        'start': start.strftime('%Y-%m-%dT00:00:00.000Z'),\n",
    "        'end': end.strftime('%Y-%m-%dT00:00:00.000Z'),\n",
    "        'mba': mba,\n",
    "    }\n",
    "    r = requests.get(f\"{ESETT_BASE}{endpoint}\", params=params, timeout=30)\n",
    "    if r.status_code == 204:\n",
    "        return None\n",
    "    r.raise_for_status()\n",
    "    return pd.DataFrame(r.json())\n",
    "\n",
    "\n",
    "def normalize_series(df: pd.DataFrame, zone: str, value_col: str) -> pd.DataFrame | None:\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    ts_col = 'timestampUTC' if 'timestampUTC' in df.columns else 'timestamp'\n",
    "    if ts_col not in df.columns or value_col not in df.columns:\n",
    "        return None\n",
    "    out = df[[ts_col, value_col]].copy()\n",
    "    out['datetime_utc'] = pd.to_datetime(out[ts_col], utc=True).dt.tz_localize(None)\n",
    "    return out[['datetime_utc', value_col]].rename(columns={value_col: zone})\n",
    "\n",
    "\n",
    "def download_esett(endpoint: str, out_name: str, value_col: str, years: list[int]) -> None:\n",
    "    for year in years:\n",
    "        dest_dir = DATASET_DIR / 'esett' / str(year)\n",
    "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "        merged: pd.DataFrame | None = None\n",
    "        for zone, mba in MBA_CODES.items():\n",
    "            zone_parts = []\n",
    "            print(f\"{out_name} | {zone} | {year}\")\n",
    "            for start, end in month_edges(year):\n",
    "                try:\n",
    "                    df = fetch_esett(endpoint, mba, start, end)\n",
    "                    df = normalize_series(df, zone, value_col)\n",
    "                    if df is not None:\n",
    "                        zone_parts.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error {zone} {start.date()}: {e}\")\n",
    "                time.sleep(1)\n",
    "            if zone_parts:\n",
    "                zone_df = pd.concat(zone_parts).drop_duplicates().sort_values('datetime_utc')\n",
    "                merged = zone_df if merged is None else merged.merge(zone_df, on='datetime_utc', how='outer')\n",
    "        if merged is not None:\n",
    "            dest = dest_dir / f\"{out_name}_{year}.csv\"\n",
    "            merged.to_csv(dest, index=False)\n",
    "            print(f\"Saved {dest} ({merged.shape})\")\n",
    "        else:\n",
    "            print(f\"No data for {out_name} {year}\")\n",
    "\n",
    "RUN_ESETT = False\n",
    "if RUN_ESETT:\n",
    "    download_esett('/EXP16/Aggregate', 'production', 'total', [2023, 2024])\n",
    "    download_esett('/EXP15/Aggregate', 'consumption', 'total', [2023, 2024])\n",
    "    download_esett('/EXP13/Aggregate', 'imbalance', 'imbalance', [2023, 2024])\n",
    "    download_esett('/EXP14/Aggregate', 'balancing_price', 'upRegPrice', [2023, 2024])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0fb40",
   "metadata": {},
   "source": [
    "## Energi Data Service (Elspot prices)\n",
    "Downloads day-ahead prices for all Nordic zones and saves to `Dataset/energi/<year>/<zone>_prices.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7dfd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elspot_prices(start: str, end: str, area: str) -> pd.DataFrame | None:\n",
    "    url = 'https://api.energidataservice.dk/dataset/Elspotprices'\n",
    "    params = {'start': start, 'end': end, 'filter': f'{{\"PriceArea\": [\"{area}\"]}}'}\n",
    "    resp = requests.get(url, params=params, timeout=30)\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"Failed {area}: {resp.text}\")\n",
    "        return None\n",
    "    data = resp.json().get('records', [])\n",
    "    if not data:\n",
    "        return None\n",
    "    df = pd.DataFrame(data)\n",
    "    df['datetime_utc'] = pd.to_datetime(df['HourUTC']).dt.tz_localize(None)\n",
    "    return df[['datetime_utc', 'SpotPriceEUR']]\n",
    "\n",
    "\n",
    "def download_elspot(year: int) -> None:\n",
    "    start, end = f\"{year}-01-01\", f\"{year+1}-01-01\"\n",
    "    out_dir = DATASET_DIR / 'energi' / str(year)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for zone in NORDIC_ZONES:\n",
    "        print(f\"Prices {zone} {year}\")\n",
    "        df = get_elspot_prices(start, end, zone)\n",
    "        if df is None:\n",
    "            print(f\"  No data {zone} {year}\")\n",
    "            continue\n",
    "        dest = out_dir / f\"{zone.lower()}_prices.csv\"\n",
    "        df.to_csv(dest, index=False)\n",
    "        print(f\"  Saved {dest}\")\n",
    "\n",
    "RUN_ELSPOT = False\n",
    "if RUN_ELSPOT:\n",
    "    for yr in [2023, 2024]:\n",
    "        download_elspot(yr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18ac0b",
   "metadata": {},
   "source": [
    "## ERA5 weather (download + extract)\n",
    "Downloads monthly ERA5 single-level data to GRIB and extracts per bidding zone to CSV in `Weather data/<year>/...`. Requires `cdsapi` credentials configured in your local CDS file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBOX = [72, 5, 54, 32]  # N, W, S, E\n",
    "POINTS = {\n",
    "    \"DK1\": (55.6, 9.2),\n",
    "    \"DK2\": (55.7, 12.5),\n",
    "    \"NO1\": (60.0, 10.0),\n",
    "    \"NO2\": (59.0, 6.5),\n",
    "    \"NO3\": (64.0, 11.0),\n",
    "    \"NO4\": (69.0, 19.0),\n",
    "    \"NO5\": (62.0, 5.5),\n",
    "    \"SE1\": (66.0, 20.0),\n",
    "    \"SE2\": (63.0, 17.0),\n",
    "    \"SE3\": (59.5, 16.0),\n",
    "    \"SE4\": (57.0, 15.0),\n",
    "    \"FI\":  (61.5, 25.0),\n",
    "}\n",
    "\n",
    "\n",
    "def download_era5_month(year: int, month: int) -> Path | None:\n",
    "    if cdsapi is None:\n",
    "        print(\"cdsapi not installed; skipping download\")\n",
    "        return None\n",
    "    raw_dir = RAW_WEATHER_DIR / str(year)\n",
    "    raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "    grib_path = raw_dir / f\"era5_weather_{year}_{month:02d}.grib\"\n",
    "    if grib_path.exists():\n",
    "        return grib_path\n",
    "    client = cdsapi.Client()\n",
    "    req = {\n",
    "        'product_type': 'reanalysis',\n",
    "        'variable': [\n",
    "            '2m_temperature','10m_u_component_of_wind','10m_v_component_of_wind',\n",
    "            'mean_sea_level_pressure','total_precipitation','100m_u_component_of_wind','100m_v_component_of_wind','surface_solar_radiation_downwards'\n",
    "        ],\n",
    "        'year': [f\"{year}\"],\n",
    "        'month': [f\"{month:02d}\"],\n",
    "        'day': [f\"{d:02d}\" for d in range(1,32)],\n",
    "        'time': [f\"{h:02d}:00\" for h in range(24)],\n",
    "        'area': BBOX,\n",
    "        'format': 'grib',\n",
    "    }\n",
    "    client.retrieve('reanalysis-era5-single-levels', req).download(str(grib_path))\n",
    "    return grib_path\n",
    "\n",
    "\n",
    "def extract_grib_to_csv(grib_path: Path) -> None:\n",
    "    ds = xr.open_dataset(grib_path, engine='cfgrib', backend_kwargs={'indexpath': ''})\n",
    "    year = grib_path.parent.name\n",
    "    month = grib_path.stem.split('_')[-1]\n",
    "    out_dir = WEATHER_OUT_DIR / str(year)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    rows = []\n",
    "    for zone, (lat, lon) in POINTS.items():\n",
    "        point = ds.sel(latitude=lat, longitude=lon, method='nearest')\n",
    "        df_zone = point.to_dataframe().reset_index()\n",
    "        df_zone['zone'] = zone\n",
    "        rows.append(df_zone)\n",
    "    df_all = pd.concat(rows, ignore_index=True)\n",
    "    keep = ['valid_time','zone'] + [c for c in ['t2m','u10','v10','msl','tp','ssrd','u100','v100'] if c in df_all.columns]\n",
    "    df_all = df_all[keep]\n",
    "    if 't2m' in df_all.columns:\n",
    "        df_all['t2m'] = df_all['t2m'] - 273.15\n",
    "    out_path = out_dir / f\"bidding_zone_weather_{year}_{month}.csv\"\n",
    "    df_all.to_csv(out_path, index=False)\n",
    "    print(f\"Saved {out_path}\")\n",
    "\n",
    "RUN_ERA5_DOWNLOAD = False\n",
    "RUN_ERA5_EXTRACT = False\n",
    "YEARS_ERA5 = [2023, 2024]\n",
    "\n",
    "if RUN_ERA5_DOWNLOAD:\n",
    "    for yr in YEARS_ERA5:\n",
    "        for mo in range(1,13):\n",
    "            download_era5_month(yr, mo)\n",
    "\n",
    "if RUN_ERA5_EXTRACT:\n",
    "    for yr in YEARS_ERA5:\n",
    "        for mo in range(1,13):\n",
    "            grib = RAW_WEATHER_DIR / str(yr) / f\"era5_weather_{yr}_{mo:02d}.grib\"\n",
    "            if grib.exists():\n",
    "                extract_grib_to_csv(grib)\n",
    "            else:\n",
    "                print(f\"Missing {grib}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
